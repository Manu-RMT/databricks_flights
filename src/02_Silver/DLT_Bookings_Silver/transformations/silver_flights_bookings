import dlt
from pyspark.sql.functions import *
from src.Communs.utils import *

src_bookings = "bookings"
bronze_path_bookings = f"{BRONZE_DATA}{src_bookings}"

@dlt.view(name="silver_flights_stage_bookings")
def silver_flights_stage_bookings():

    df_bookings = spark.readStream.format("delta").load(bronze_path_bookings)
    return df_bookings

@dlt.view(name="transform_bookings")
def transform_bookings():

    df = spark.readStream.table("silver_flights_stage_bookings")
    df = df.withColumn("booking_date", to_date(col("booking_date"),"yyyy-MM-dd")) \
           .withColumn("amount", col("amount").cast("double")) \
           .withColumn("modifiedDate", current_timestamp()) \
           .drop("_rescued_data")

    return df

rules = {
"rule1": "booking_id IS NOT NULL",
"rule2": "passenger_id IS NOT NULL",
"rule3": "flight_id IS NOT NULL"
}
 
@dlt.table(name="silver_flights_bookings")
@dlt.expect_all_or_drop(rules)
def silver_flights_bookings():
    
   return spark.readStream.table("transform_bookings")
   