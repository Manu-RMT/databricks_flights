{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72475bf5-deaa-4024-b585-b74847b449ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **INCREMENTAL DATA INGESTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "928827d9-4492-4acf-b04a-15593915d29b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Enabling Autoreload for Python Module Updates"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b37d2a4-db42-4f1d-be86-2c5ee4061f9b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Importing Configuration and Updating Python Path"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# -----------------------------\n",
    "# Ajouter le repo au Python Path\n",
    "# -----------------------------\n",
    "sys.path.append(\"/Workspace/Users/mandu543@gmail.com/databricks_flights/Pipelines\")\n",
    "\n",
    "from lib.config import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d2f21b3-37ea-48b2-86cb-ee46aa1ddbd1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initializing Bronze Silver and Gold Spark Volumes"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f'CREATE VOLUME IF NOT EXISTS {RAW_BRONZE_ZONE}');\n",
    "spark.sql(f'CREATE VOLUME IF NOT EXISTS {RAW_SILVER_ZONE}');\n",
    "spark.sql(f'CREATE VOLUME IF NOT EXISTS {RAW_GOLD_ZONE}');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78d015e1-335c-4e90-87af-fe380e8dfa59",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating Spark Volumes for Data Storage Tiers"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f'CREATE VOLUME IF NOT EXISTS {BRONZE_ZONE}.bronzeVolume')\n",
    "spark.sql(f'CREATE VOLUME IF NOT EXISTS {SILVER_ZONE}.silverVolume')\n",
    "spark.sql(f'CREATE VOLUME IF NOT EXISTS {GOLD_ZONE}.goldVolume')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e33b9c0-a0e6-40c8-81e6-d3322ebd6e09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Ingestion Data with Loop (Jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d880122d-0c99-4e2a-8707-a3cd344c04d5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Centralized Data Path and Table Setup"
    }
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# PARAMETRAGE CENTRALISE\n",
    "# =========================================================\n",
    "\n",
    "# Bonnes pratiques :\n",
    "# - Centraliser tous les chemins et noms de table en haut du notebook\n",
    "# - Facilite la maintenance et la promotion DEV → TEST → PROD\n",
    "# - Permet d'utiliser des variables dynamiques (widgets, env, etc.)\n",
    "# - Evite le hardcoding partout dans le code\n",
    "# =========================================================\n",
    "\n",
    "\n",
    "# Bookings, Airports, Flights, Customers\n",
    "dbutils.widgets.text(\"src_data\",\"\")\n",
    "source_data = dbutils.widgets.get(\"src_data\")\n",
    "\n",
    "source_data_lower = source_data.lower()\n",
    "raw_path = f\"/Volumes/workspace/raw_flights/raw_data/{source_data}/\"\n",
    "bronze_path = f\"/Volumes/workspace/01_bronze/bronzevolume/{source_data_lower}/data\"\n",
    "schema_path = f\"/Volumes/workspace/01_bronze/bronzevolume/{source_data_lower}/schema_tracking\"\n",
    "checkpoint_path = f\"/Volumes/workspace/01_bronze/bronzevolume/{source_data_lower}/stream_checkpoint\"\n",
    "table_name = BRONZE_ZONE + f\".flights_{source_data_lower}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba8a0f3f-d66d-4b9b-963c-c35b262f86b5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Automatic CSV Ingestion Using CloudFiles and Schema Res ..."
    }
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# LECTURE AUTO LOADER\n",
    "# =========================================================\n",
    "\n",
    "# Objectif :\n",
    "# - Ingestion incrémentale automatique des fichiers CSV\n",
    "# - Détection automatique des nouveaux fichiers\n",
    "# - Gestion de l’évolution du schéma\n",
    "#\n",
    "# Bonnes pratiques Bronze :\n",
    "# - Utiliser schemaEvolutionMode = \"rescue\" (ne jamais casser le pipeline)\n",
    "# - Séparer schema tracking et checkpoint streaming\n",
    "# - Activer inferColumnTypes pour éviter tout en string\n",
    "# - includeExistingFiles = true uniquement au premier run\n",
    "# =========================================================\n",
    "\n",
    "df_raw = (\n",
    "    spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.schemaLocation\", schema_path)\n",
    "        .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .option(\"cloudFiles.includeExistingFiles\", \"true\")\n",
    "        .load(raw_path)\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e2a792c-13ac-4ace-9f55-6ce5a133ba89",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Streaming Delta Write for Bronze Layer Bookings Data"
    }
   },
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# ECRITURE DELTA (Bronze Layer)\n",
    "# Bonnes pratiques :\n",
    "# - Append only en Bronze\n",
    "# - Checkpoint séparé du schema\n",
    "# - trigger(once=True) pour batch incrémental orchestré\n",
    "# - awaitTermination() pour contrôle propre du job\n",
    "# =====================================\n",
    "\n",
    "requete = (\n",
    "    df_raw.writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .trigger(once=True)\n",
    "        .option(\"checkpointLocation\",\n",
    "                checkpoint_path)\n",
    "        .option(\"path\",\n",
    "                bronze_path)\n",
    "        .start()\n",
    ")\n",
    " \n",
    "requete.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07a7e884-082f-4a46-bb0c-03ce74bcfbd6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fetching Data from Delta Table in Bronze Storage Layer"
    }
   },
   "outputs": [],
   "source": [
    "table = spark.sql(f\" SELECT * from delta.`{bronze_path}`\")\n",
    "display(table)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_BonzeLayer",
   "widgets": {
    "src_data": {
     "currentValue": "",
     "nuid": "0d6cf137-7d45-4f7f-b56e-5335462c0ef3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "src_data",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "src_data",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
